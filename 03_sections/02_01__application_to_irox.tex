%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Active Learning Machine Learning Results %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Notes:
%   - QUESTION Is there any physical intuition that we can gleam from the
% fingerprints?
%   - QUESTION What did @Chris mean by this? "Computed amorphous phase to
% define synthesizability"
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% ################################# Paragraph #################################
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Intro/transition paragraph
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% | - Paragraph start
%
We now turn our attention of the application of this active learning scheme to the discovery of most stable forms of \IrOtwo and \IrOthree.
%
The algorithm is applied separately to both stoichiometries.
%
Here, we report in detail the results for \IrOthree, the analogous results for  \IrOtwo are shown in more detail in the SI.
% __|


% ################################# Paragraph #################################
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Results for IrO3
% ALL results for IrO3
%   * Introduce the convergence plots
%   * The GP becomes more accurate as more DFT is aquired
%   * The GP can start to recognize the low energy systems after minimal DFT
%   *
% We need to call them something else than "convergence plots" (bad name)
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% | - Paragraph start
%
Figure \ref{fig:iro2_al}a shows a sequence of plots at various generations of the active learning loop,
starting with the initial generation with five randomly drawn candidates and ending with the 40th generation of the ALL
(205  DFT computed structures out of the 248 total candidates).
%
Each plot tracks the predicted (hollow grey) and DFT-derived (solid red) formation enthalpies, sorted from most to least stable.
%
As the active learning loop acquires DFT data, the GP model becomes more accurate, as evidenced by the decreasing uncertainties when going from left to right.
%
At the top of each subplot of \ref{fig:iro2_al} the identity of top ten most stable polymorphs is tracked,
with the short grey line turning into a longer red line when the structure is acquired by the ALL.
%
At the first generation the top ten structures are randomly distributed across the entire candidate space because the GP model hasn't had enough training data to identify the most stable polymorphs as low energy systems.
%
After only three generations (\ref{fig:iro2_al}a.ii) the GP model is sufficiently trained to have correctly identified all of the top ten systems as being low energy.
%
Additionally, by the third generation (20 DFT calculations) 2/10 top systems have been acquired.
%
After another 3 generations (\ref{fig:iro2_al}a.iii) (15 additional DFT calculations) the AL routine has successfully identified 7/10 of the top systems.
%
Figure \ref{fig:iro2_al}e plots the number of top ten structures acquired as a function of DFT calculations for the ALL with the GP-UCB acquisition function and a baseline random acquisition scheme.
%
The results of figure \ref{fig:iro2_al}e are averaged over independent runs of AL algorithm with the 1 sigma standard deviation between these runs shown.
%
Overall, the GP-UCB runs outperform the random acquisition runs, with only 50 DFT calculations on average needed to discover 7 of the top 10 systems.
%
This is compared to the 157 DFT calculations needed to compare 7/10 top systems for the random acquisition.
% __|

% TEMP | Find a place for these sentences where they flow
%
The low energy region of \ref{fig:iro2_al}a.iii is shown in \ref{fig:iro2_al}c.
%
And the 6 most stable structural polymorphs are shown in \ref{fig:iro2_al}d.


% ################################# Paragraph #################################
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Discussion on performance of the AL routine
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% | - Paragraph start
Having demonstrated the efficacy of the ALL materials discovery scheme we will now turn our attention to evaluating the performance of the GP regressive model of the final generation (trained on all available \IrOthree DFT data).
%
Figure \ref{fig:iro2_al}b plots the GP model predicted formation enthalpy against the DFT-computed values for two special cases,
1.) predicting onto the features of the pre-optimized structures (grey), as is done in the regular operation of the ALL when acquiring new structures and
2.) predicting onto the post-DFT fingerprints (blue).
%
It is evident from the parity plot in figure \ref{fig:iro2_al}b that the Gaussian process model is doing an poor job of predicting the DFT formation energy of the candidate space using the pre-optimized fingerprints,
with an exceptionally poor MAE of ~1.5 eV/atom.
%
The model's poor predictions are skewed towards the higher formation enthalpies, with the errors associated with low energy structures being much more robust.
%
The same GP model does comparatively much better at predicting the formation energies of post-DFT optimized structures with an MAE of ~0.2 eV/atom,
which is expected since the post-DFT fingerprints directly corresponds to the DFT energies.
%
This is to show that the GP model's poor predictive capabilities is due to the large degree of structural reorganization that occurs after DFT relaxation of the pre-optimized structure.
%
Structures that are initialized in high energy configurations will therefore have high predicted formation enthalpies, and will then reconfigure into a lower nearby configuration, resulting in lower final energy and a large discrepancy between the predicted and final energies.
%
It's interesting to consider why the ALL appears to perform so well as outlined previously (discovering 7/10 of the most stable candidates after only 35 DFT calculations).
%
The reason for this is that the pre-optimized structures that are similar enough to the most stable final equilibrium structures will not restructure considerably, meaning that their predicted formation energies will be close enough (and low enough) to be quickly picked up by the acquisition criteria.
%
% __|



% | - Figure | IrO3 Convergence Plot
\begin{figure*}[!htb]
\centering
\makebox[\textwidth][c]{\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]
{02_figures/ml_convergence_plots/test_iro3_al.png}
}
\caption{\label{fig:iro2_al}
% (a) %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
(a) Progress of the active learning algorithm at five subsequent generations.
%
The AL generation and number of DFT training data at each generation is shown for each subplot.
%
The enthalpy of formation is plotted, ordered from most to least stable, against all \IrOthree candidates.
%
Grey markers indicate predicted formation enthalpies from the ML model while red markers correspond to DFT-computed quantities.
%
Error bars from the GP model corresponding to 1 sigma are shown for all predictions.
%
The vertical lines at the top of each subplot are tracking the positions of the 10 most stable polymorphs at each generation and whether they have been acquired (red) or not (grey) by the AL routine.
% (b) %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
(b) Parity plot of the final ML models for \IrOtwo and \IrOthree predicting on either the pre-optimized (grey) or the post-optimized structures of \IrOtwo and \IrOthree.
% (c) %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
(c) Zoomed inset of the 6th generation of the AL loop.
% (d) %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
(d) Crystal structures of 6 most stable \IrOthree polymorphs.
% (e) %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
(e) The number of the top 10 most stable polymorphs of \IrOthree that are discovered as a function of the number of DFT bulk relaxations, averaged over 5 independent runs of the AL algorithm using the GP-UCB acquisition criteria (red) and a random acquisition method (grey).
%
Error bars indicate the standard deviation over 5 runs.
%
Red guide lines are displayed to show how many DFT calculations are needed to discover 7/10 of most stable polymorphs for the GP-UCB and random acquisition.
}
\end{figure*}
% __|



% | - __old__
% This is clearly seen in the parity plot (Fig TEMP), which shows that the predicted DFT energies become increasingly less accurate as stability of the polymorph decreases, with low stability structures tending to reconfigure into lower energy states.
% %
% This is significantly lower than the leave-one out cross validation error of ~0.15 eV/atom, which is performed by predicting only on optimized strctures, indicating that the issue does not lie with a poor predictive model, but instead with the large degree of structural reorganization that occurs over the course of a typical DFT ionic optimization.
% %
% % This is because the candidate space structures are generated
% % To compare the performance of the AL routine we compare against a
% %
% Figure subplot TEMP shows the parity plot of the predicted DFT energies using the fingerprints corresponding to the unoptimized structures (COLOR1) and post-DFT relaxed structures (COLOR2),
% plotted against the final post-optimization DFT formation energy.
% % Sentence that explains the parity plot
% It is clear that the GP model severely underpredicts the energy of the candidate space, especially for systems with less stable predicted formation energies, when predicting onto the candidate space of unoptimized structures (MAE of ~0.5 eV/atom).
% %
% This is due to the fact that the structures undergo through a large degree of structural rearrangement over the course of the DFT relaxation.
% %
% Alternatively, the MAE is much lower when the model predicts onto the post-optimized structures (MAE ~0.15 eV/atom),
% demonstrating that the poor preditive capabilities is due primarily to structural drift and not to the poor predictive capabilities of the GP model.
%
% Despite these shortcomings, figure TEMP (TEMP performance plot) tracks the number of high stability materials (lowest N structures) discovered as a function of the number of DFT calculations for the case of 1.) AL with UCB acquisition function and 2.) AL with a random acquisition function.
% %
% The random acquisition function emulates the behavior of regression model which exhibits no correlation between the input and output space.
% %
% Clearly, the rate of discovery for case 1.) is superior to that of case 2.), with our methodology discovering half of the lowest N structures after only TEMP DFT calculations compared to TEMP when acquiring randomley within the candidate space.
%
% The middle panel is reproduced in figure \ref{fig:iro2_al}, which also shows the real DFT energies accompanying each predicted formation energy (hollow diamonds).
% Probably remove the diamonds and add the parity subplot?
% Figure \ref{fig:iro2_al} b. shows the formation energy, either predicted (small colored circles) or computed (large red-bordered circles), is plotted on the y-axis against all structures in the candidate space of \IrOthree polymorphs.
%
% As seen from Figure\ref{fig:iro2_al} a) the early iterations, where the training data is sparse
%
% Figure \ref{fig:iro2_al} a. shows the progression of the model at selected generations of the active learning loop, starting and ending with the initially seeded generation and final generation, respectively.
%
% An acquisition bin size of 10 DFT calculations per iteration and an initial seed of 11 calculations were used.
% plots of the AL predictions on the candidate space at particular generations of the AL algorithm for the \IrOthree candidate space with an aquisition bin size of 10 DFT calculations per iteration and an initial 11 calculations used for seeding.
% The AL algorithm is applied separately to \IrOtwo and \IrOthree because we are specifically interested in the most stable polymorphs at each stoichiometry.
% TODO How many features drop out when you constrain the stoich.?
% This has an the added benefit of reducing the number of fingerprint vectors from the Voronoi tesselation dramatically (from TEMP to TEMP) because many features are senstive to changes in stoichiometry.
% __|
